import os
import pickle
import random
import tempfile
from typing import Literal, Tuple

import mlflow
import numpy as np
import torch
from mlflow.entities import DatasetInput

from adv_error_metrics import (
    adv_absolute_regret_error,
    adv_fooling_relative_regret_error,
    adv_relative_regret_error,
)


def data_module_to_mlflow_dataset_input(data) -> Tuple[DatasetInput, DatasetInput, DatasetInput]:
    """Returns a list of DatasetInput objects from the DataModule object such that these
    objects can be used in the mlflow API.

    :param data: the data module object
    :type data: DataModule
    :return: the DatasetInput objects
    :rtype: Tuple[DatasetInput, DatasetInput, DatasetInput]
    """
    # First get the classname of the data object
    train_mlflow_dataset = mlflow.data.from_numpy(
        features=data.train_df.x,
        targets={"z": data.train_df.y, "sol": data.train_df.sol},
        name="Train Dataset",
    )
    train_dataset_inp = DatasetInput(dataset=train_mlflow_dataset._to_mlflow_entity())
    # test
    test_mlflow_dataset = mlflow.data.from_numpy(
        features=data.test_df.x,
        targets={"z": data.test_df.y, "sol": data.test_df.sol},
        name="Test Dataset",
    )
    test_dataset_inp = DatasetInput(dataset=test_mlflow_dataset._to_mlflow_entity())
    # valid
    valid_mlflow_dataset = mlflow.data.from_numpy(
        features=data.valid_df.x,
        targets={"z": data.valid_df.y, "sol": data.valid_df.sol},
        name="Valid Dataset",
    )
    valid_dataset_inp = DatasetInput(dataset=valid_mlflow_dataset._to_mlflow_entity())
    return train_dataset_inp, test_dataset_inp, valid_dataset_inp


def get_best_model_path(artifact_path: str) -> str:
    """Returns the best model path from the artifact path
    :param artifact_path: The artifact path where all the checkpoints are stored
    :type artifact_path: str
    """
    # cycle through all the directories and check all files that are called aliases.txt
    # if the file contains the word best, then return the directory
    for root, _, files in os.walk(artifact_path):
        for file in files:
            if file == "aliases.txt":
                with open(os.path.join(root, file), "r") as f:
                    if "best" in f.read():
                        # Now return the only .ckpt file
                        return os.path.join(root, [f for f in files if f.endswith(".ckpt")][0])
    raise FileNotFoundError(
        f"No best model found in {artifact_path}. Please check the artifact path and the aliases.txt files."
    )


def seed_all(seed: int) -> torch.Generator:
    """Seed all the random number generators"""
    torch_generator = torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.cuda.manual_seed(seed)
    # TODO: Legacy -> change to np.random.Generator
    np.random.seed(seed)
    random.seed(seed)
    return torch_generator


def get_image_and_corresponding_path(img_array, path_array) -> np.ndarray:
    # check if the img data has the correct shape
    # Otherwise transpose
    if img_array.shape[0] == 3:
        img_array = img_array.transpose(1, 2, 0)
    # Step 1: Overlay path on the original image
    img_with_path = img_array.copy()
    block_size = 8
    # Loop over the 12x12 grid and highlight the path in the original image
    for i in range(path_array.shape[0]):
        for j in range(path_array.shape[0]):
            if path_array[i, j] == 1:
                # Highlight the 8x8 block in red on the original image
                img_with_path[
                    i * block_size : (i + 1) * block_size, j * block_size : (j + 1) * block_size, :
                ] = [255, 0, 0]  # Red for path

    return img_with_path


def get_transferability_data_from_mlflow(run_id_attacker: str) -> dict:
    """Returns the transferability data from the mlflow run with the given run_id

    Args:
        run_id_attacker (str): The attacker for which to get the data

    Returns:
        dict: A dict where the keys are the run ids on which the set of examples generated by the
        attacker are evaluated on. Each dict contains a tuple with one entry for metrics and the
        other for decisions. For the metrics there is another dict where the metric is the key and
        the values is a list of metric values for each example.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        path = mlflow.artifacts.download_artifacts(
            run_id=run_id_attacker,
            artifact_path="cross_evaluation_results.pkl/cross_evaluation_results.pkl",
            dst_path=tmpdir,
        )
        evaluation_results = pickle.load(open(path, "rb"))
        # here we are interested in the evaluation on the same model
        return evaluation_results


def get_attacker_regret_errors(attacker_run_id: str, problem: str):
    minimize = get_min_or_max_problem(problem)
    # First download the data
    with tempfile.TemporaryDirectory() as tmpdir:
        path = mlflow.artifacts.download_artifacts(
            run_id=attacker_run_id,
            artifact_path="adv_samples/adv_samples.npz",
            dst_path=tmpdir,
        )
        adv_samples = np.load(open(path, "rb"))
        path = mlflow.artifacts.download_artifacts(
            run_id=attacker_run_id,
            artifact_path="adv_samples/adv_decisions.npz",
            dst_path=tmpdir,
        )
        adv_decisions = np.load(open(path, "rb"))
    # Now compute the relative regrets and regrets
    c = adv_samples["c"]
    dec = adv_samples["dec"]
    dec_hat = adv_decisions["dec_hat"]
    dec_adv_hat = adv_decisions["dec_adv_hat"]
    assert len(c) == 139 or len(c) == 1000, f"Test set has weird length of {len(c)}, check this"

    # Reshaping -> if 2d data
    c = c.reshape(c.shape[0], -1)
    dec = dec.reshape(dec.shape[0], -1)
    dec_hat = dec_hat.reshape(dec_hat.shape[0], -1)
    dec_adv_hat = dec_adv_hat.reshape(dec_adv_hat.shape[0], -1)

    rre_clean = adv_relative_regret_error(
        c=c,
        dec_adv=dec,
        dec_adv_hat=dec_hat,  # we only want the normal regret and not of the adv
        minimize=minimize,
    )
    are_clean = adv_absolute_regret_error(
        c=c,
        dec_adv=dec,
        dec_adv_hat=dec_hat,  # we only want the normal regret and not of the adv
        minimize=minimize,
    )
    rre_adv = adv_relative_regret_error(
        c=c,
        dec_adv=dec,
        dec_adv_hat=dec_adv_hat,  # this time we want the adv decisions
        minimize=minimize,
    )
    are_adv = adv_absolute_regret_error(
        c=c,
        dec_adv=dec_adv_hat,
        dec_adv_hat=dec_adv_hat,  # this time we want the adv decisions
        minimize=minimize,
    )
    frre_adv = adv_fooling_relative_regret_error(
        c=c,
        dec_adv=dec,
        dec_hat=dec_hat,
        dec_adv_hat=dec_adv_hat,  # this time we want the adv decisions
        minimize=minimize,
    )

    return rre_clean, are_clean, rre_adv, are_adv, frre_adv


def get_threshold_val(error_metric_arr: np.ndarray, threshold: float) -> float:
    """Returns the percentage of elements in the array that are bigger or equal than the given
    threshold

    Args:
        error_metric_arr (np.ndarray): The array containing the error metric values
        threshold (float): The threshold

    Returns:
        float: The percentage
    """
    return np.sum(error_metric_arr > threshold) / len(error_metric_arr)


def get_min_or_max_problem(
    problem: Literal["Warcraft", "Knapsack", "ShortestPath"],
) -> bool:
    """Returns True if the problem is a minimization problem and False if it is a maximization"""
    if problem in ["Warcraft", "ShortestPath"]:
        return True
    elif problem in ["Knapsack"]:
        return False
    else:
        raise ValueError(
            f"Problem {problem} is not supported. Supported problems are: "
            "Warcraft, Knapsack, ShortestPath"
        )
