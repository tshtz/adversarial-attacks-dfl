{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR\"] = \"False\"\n",
    "import mlflow \n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sys\n",
    "from helpers import get_transferability_data_from_mlflow, get_threshold_val, get_attacker_regret_errors\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_ATTACKER_ORDER = [\"clean\", \"random\", \"fgsm_pred_ls\", \"fgsm_decision\", \"fgsm_custom\", \"apgd_pred_l2\", \"apgd_decision\", \"apgd_custom\"]\n",
    "model_name_dict = {\n",
    "    'baseline_mse' : \"PF\",\n",
    "    'SPO' : \"SPO\",\n",
    "    'DBB' : \"DBB\",\n",
    "    \"IMLE\" : \"IMLE\",\n",
    "    'FenchelYoung' : \"FY\",\n",
    "    \"DCOL\" : \"QPTL\",\n",
    "    'IntOpt' : \"IntOpt\",\n",
    "    'CachingPO_listwise' : \"Listwise\",\n",
    "    'CachingPO_pairwise' : \"Pairwise\",\n",
    "    'CachingPO_pairwise_diff' : \"PairwiseDiff\",\n",
    "    'CachingPO_MAP_c' : \"MAP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create a column for the attacker_sig\n",
    "# Helper function to safely get parameter values\n",
    "def safe_get_param(row, param_name, default=\"None\"):\n",
    "    if param_name in row and pd.notna(row[param_name]):\n",
    "        return str(row[param_name])\n",
    "    return default\n",
    "\n",
    "def format_str_to_same_length(inp1, inp2, inp3, padding1, padding2, padding3):\n",
    "    len_1 = len(inp1)\n",
    "    len_2 = len(inp2)\n",
    "    len_3 = len(inp3)\n",
    "    # pad all of them using _\n",
    "    inp1 = inp1 + \"_\" * (padding1 - len_1)\n",
    "    inp2 = inp2 + \"_\" * (padding2 - len_2)\n",
    "    inp3 = inp3 + \"_\" * (padding3 - len_3)\n",
    "    # return one string\n",
    "    return f\"{inp1}_{inp2}_{inp3}\"\n",
    "\n",
    "def insert_spacers(z, labels, group_keys):\n",
    "    new_z = []\n",
    "    new_labels = []\n",
    "    prev_key = None\n",
    "\n",
    "    group_id = 0\n",
    "    for i, (row, label, key) in enumerate(zip(z, labels, group_keys)):\n",
    "        if prev_key is not None and key != prev_key:\n",
    "            new_z.append([np.nan] * z.shape[1])  # insert blank row\n",
    "            new_labels.append(f\"Group: {group_id+1}\")                # blank label\n",
    "            group_id += 1\n",
    "        new_z.append(row)\n",
    "        new_labels.append(label)\n",
    "        prev_key = key\n",
    "\n",
    "    return np.array(new_z), new_labels\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    dataset_name = dataset_name + \"_Models\"\n",
    "    # TODO: Adjust to your experiment id\n",
    "    experiment_id = \"228319214249946667\"\n",
    "    # Get all the models with img_size 12 -> get a list of run ids\n",
    "    runs = mlflow.search_runs(experiment_ids=experiment_id)\n",
    "    # Filter for finished\n",
    "    runs = runs[runs[\"status\"] == \"FINISHED\"]\n",
    "    # Filter for dataset_name\n",
    "    runs = runs[runs[\"params.attacked_models_experiment\"] == dataset_name]\n",
    "    # Assert that all models have been attacked\n",
    "    runs[\"attacker_sig\"] = runs.apply(lambda row: \n",
    "        row[\"params.attacker\"] + \n",
    "        \"_e\" + safe_get_param(row, \"params.epsilon\") +\n",
    "        \"_a\" + safe_get_param(row, \"params.alpha\") +\n",
    "        \"_m\" + safe_get_param(row, \"params.max_iter\") +\n",
    "        \"_r\" + safe_get_param(row, \"params.restarts\") +\n",
    "        (\"_s\" + safe_get_param(row, \"params.use_signed_grad\") if \"params.use_signed_grad\" in row else \"\"), \n",
    "        axis=1)\n",
    "    # In case of warcraft change the attacked_models_name\n",
    "    if dataset_name == \"Warcraft_Models\":\n",
    "        runs[\"params.attacked_models_name\"] = runs[\"params.attacked_models_name\"].apply(lambda x: x.replace(\"_regret\", \"\"))\n",
    "    runs[\"model_epsilon\"] = runs[\"params.attacked_models_name\"].astype(str) + \"_eps=\" + runs[\"params.epsilon\"].astype(str)\n",
    "    print(f\"Found {len(runs)} runs for {dataset_name}\")\n",
    "    return runs\n",
    "\n",
    "def validate_attacker_for_epsilon(data):\n",
    "    # In this case we do not care about the random noise attacker\n",
    "    data = data[data[\"params.attacker\"] != \"random_noise\"]\n",
    "    # for each attacker check that rre and frre increase with each epsilon increases\n",
    "    attackers = data[\"params.attacker\"].unique()\n",
    "    for attacker in attackers:\n",
    "        print(f\"Evaluating attacker: {attacker}\")\n",
    "        attacker_data = data[data[\"params.attacker\"] == attacker]\n",
    "        # now check for each model\n",
    "        models = attacker_data[\"params.attacked_models_name\"].unique()\n",
    "        for model in models:\n",
    "            # Now filter \n",
    "            model_data = attacker_data[attacker_data[\"params.attacked_models_name\"] == model]\n",
    "            # Sort based on epsilon\n",
    "            model_data = model_data.sort_values(\"params.epsilon\")\n",
    "            frre = model_data[\"metrics.mean_fool_rel_regret\"].values\n",
    "            rre = model_data[\"metrics.mean_rel_regret\"].values\n",
    "            acc = model_data[\"metrics.mean_acc_error\"].values\n",
    "            facc = model_data[\"metrics.mean_fool_error\"].values\n",
    "            epsilons = model_data[\"params.epsilon\"].values\n",
    "            # Check if frre and rre are increasing\n",
    "            for i in range(1, len(frre)):\n",
    "                if frre[i] < frre[i-1]:\n",
    "                    print(f\"Frre is not increasing for {attacker} on {model} for epsilon {model_data['params.epsilon'].values[i]}\")\n",
    "                    print(f\"Frre: (eps: {epsilons[i]}) {frre[i]} < {frre[i-1]} (eps: {epsilons[i-1]})\")\n",
    "                    print(\"COMPARING ACCURACY\")\n",
    "                    print(f\"Acc: (eps: {epsilons[i]}) {acc[i]} : {acc[i-1]} (eps: {epsilons[i-1]})\")\n",
    "                if rre[i] < rre[i-1]:\n",
    "                    print(f\"Rre is not increasing for {attacker} on {model} for epsilon {model_data['params.epsilon'].values[i]}\")\n",
    "                    print(f\"Rre: (eps: {epsilons[i]}) {rre[i]} < {rre[i-1]} (eps: {epsilons[i-1]})\")\n",
    "                    print(\"COMPARING ACCURACY\")\n",
    "                    print(f\"Acc: (eps: {epsilons[i]}) {acc[i]} : {acc[i-1]} (eps: {epsilons[i-1]})\")\n",
    "                    print(f\"Facc: (eps: {epsilons[i]}) {facc[i]} : {facc[i-1]} (eps: {epsilons[i-1]})\")\n",
    "                    print(\"-\" * 20)\n",
    "        print(\"*\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE DATASET HERE\n",
    "\n",
    "DATASET = \"Warcraft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(DATASET)\n",
    "\n",
    "rel_metrics = [\"test_regret\", \"test_mse\"]\n",
    "attacked_models_run_ids = data[\"params.attacked_models_run_id\"].unique()\n",
    "\n",
    "modelnames = []\n",
    "metric_vals  = []\n",
    "\n",
    "for metric in rel_metrics:\n",
    "    # Create one plot for each \n",
    "    for run_id in attacked_models_run_ids: \n",
    "        # get the data \n",
    "        model_run = mlflow.get_run(run_id) \n",
    "        model_name = model_run.data.params[\"modelname\"]\n",
    "        loss = model_run.data.params.get(\"loss\", None)\n",
    "        if loss is not None:\n",
    "            unique_name = f\"{model_name}_{loss}\"\n",
    "        else:\n",
    "            unique_name = model_name\n",
    "        modelnames.append(unique_name)\n",
    "        # Get the metric values \n",
    "        metric_vals.append(model_run.data.metrics[metric])\n",
    "    # Create the plot\n",
    "    fig = px.bar(\n",
    "        x=modelnames,\n",
    "        y=metric_vals,\n",
    "        title=f\"Metric: {metric}\",\n",
    "        labels={\"x\": \"Model\", \"y\": metric},\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot will show the mean performance for the given metric and epsilon for each model and for the different attackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THIS \n",
    "metric = \"metrics.mean_rel_regret\" # metrics.mean_fool_rel_regret\n",
    "data = load_data(DATASET)\n",
    "\n",
    "grouped = data.groupby(\"model_epsilon\")\n",
    "for model_eps, group in grouped:\n",
    "    assert group[\"params.attacked_models_run_id\"].nunique() == 1, f\"Expected only one run id for {model_eps}, but got {group['params.attacked_models_run_id'].nunique()}\"\n",
    "    # Get the values\n",
    "    run_id_one_attacker = group[\"run_id\"].values[0]\n",
    "    # Get the\n",
    "    rre_clean, _, _,_, _  =  get_attacker_regret_errors(run_id_one_attacker, problem=DATASET)\n",
    "    # Get the mean \n",
    "    mean_rre_attacked_model = np.mean(rre_clean)\n",
    "    # Now append a row to the dataframe \n",
    "    for epsilon in group[\"params.epsilon\"].unique():\n",
    "        params = {\n",
    "            \"params.epsilon\": epsilon,\n",
    "            \"params.attacker\": \"Clean Model\",\n",
    "            \"params.attacked_models_name\": group[\"params.attacked_models_name\"].values[0],\n",
    "            \"metrics.mean_rel_regret\": mean_rre_attacked_model\n",
    "        }\n",
    "        # Append the row to the dataframe\n",
    "        data = pd.concat([data, pd.DataFrame([params])], ignore_index=True)\n",
    "\n",
    "existing_models = [model for model in model_name_dict if model in data[\"params.attacked_models_name\"].unique()]\n",
    "# Create model positions based on your desired order\n",
    "model_positions = {model: i for i, model in enumerate(existing_models)}\n",
    "\n",
    "# Create aliases for display\n",
    "model_aliases = [model_name_dict[model] for model in existing_models]\n",
    "\n",
    "\n",
    "# Create attacker signature with only parameters that exist\n",
    "data[\"attacker_sig\"] = data.apply(lambda row: \n",
    "    row[\"params.attacker\"] + \n",
    "    \"_e\" + safe_get_param(row, \"params.epsilon\") +\n",
    "    \"_a\" + safe_get_param(row, \"params.alpha\") +\n",
    "    \"_m\" + safe_get_param(row, \"params.max_iter\") +\n",
    "    \"_r\" + safe_get_param(row, \"params.restarts\") +\n",
    "    (\"_s\" + safe_get_param(row, \"params.use_signed_grad\") if \"params.use_signed_grad\" in row else \"\") + \n",
    "    # also add the params.initial_window_size if it exists\n",
    "    (\"_i\" + safe_get_param(row, \"params.initial_window_size\") if \"params.initial_window_size\" in row else \"\"),\n",
    "    axis=1)\n",
    "\n",
    "# Budget function\n",
    "def get_budget(row):\n",
    "    # check which attacker is used \n",
    "    if row[\"params.attacker\"] == \"Clean Model\":\n",
    "        return \"clean\"\n",
    "    elif (\"fgsm\" in row[\"params.attacker\"]) or (\"random\" in row[\"params.attacker\"]) or (\"Iterative\" in row[\"params.attacker\"]):\n",
    "        return \"medium\"\n",
    "    elif (\"apgd\" in row[\"params.attacker\"]):\n",
    "        if row[\"params.max_iter\"] == \"200\":\n",
    "            return \"high\"\n",
    "        elif row[\"params.max_iter\"] == \"100\":\n",
    "            return \"medium\"\n",
    "        elif row[\"params.max_iter\"] == \"10\":\n",
    "            return \"low\"\n",
    "    else:\n",
    "        return \"medium\"\n",
    "\n",
    "# Add the budget column to the dataframe\n",
    "data[\"budget\"] = data.apply(get_budget, axis=1)\n",
    "\n",
    "# Sort data by epsilon to ensure proper ordering in facets\n",
    "data[\"params.epsilon\"] = pd.to_numeric(data[\"params.epsilon\"], errors='coerce')\n",
    "data = data.sort_values(\"params.epsilon\")\n",
    "\n",
    "# Define offsets for each budget level\n",
    "budget_offsets = {\"low\": -0.2, \"medium\": 0.0, \"high\": 0.2, \"clean\": 0.0}\n",
    "\n",
    "\n",
    "# Create the actual x-positions with offsets\n",
    "data[\"x_position\"] = data.apply(lambda row: \n",
    "    model_positions[row[\"params.attacked_models_name\"]] + budget_offsets[row[\"budget\"]], \n",
    "    axis=1)\n",
    "\n",
    "# Define symbols for each budget level (including clean)\n",
    "budget_symbols = {\"low\": \"triangle-left\", \"medium\": \"circle\", \"high\": \"triangle-right\", \"clean\": \"x\"}\n",
    "data[\"symbol\"] = data[\"budget\"].map(budget_symbols)\n",
    "\n",
    "# Create a scatter plot\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=\"x_position\",  # Use jittered x-positions\n",
    "    y=metric,\n",
    "    facet_col=\"params.epsilon\",\n",
    "    color=\"params.attacker\",\n",
    "    symbol=\"budget\",\n",
    "    hover_name=\"attacker_sig\",\n",
    "    symbol_map=budget_symbols,\n",
    "    size_max=120, \n",
    ")\n",
    "\n",
    "# Update traces to make Clean Model black\n",
    "for trace in fig.data:\n",
    "    if 'Clean Model' in trace.name:\n",
    "        trace.marker.color = 'black'\n",
    "\n",
    "# Update x-axis to show model names instead of numeric positions\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickmode='array',\n",
    "    tickvals=list(model_positions.values()),\n",
    "    ticktext=model_aliases,  # Use aliases here\n",
    "    title=\"Models\"\n",
    ")\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    height=1300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE\n",
    "epsilon = \"8\"\n",
    "data = load_data(DATASET)\n",
    "\n",
    "# Filter the data once\n",
    "data = data[data[\"params.epsilon\"] == epsilon]\n",
    "data = data[(data[\"params.max_iter\"] == \"100\") | (data[\"params.max_iter\"].isna()) | (data[\"params.max_iter\"] == \"50\")]\n",
    "data = data[data[\"params.attacker\"].str.contains(\"apgd\") | data[\"params.attacker\"].str.contains(\"argeted\")]\n",
    "\n",
    "# Define the model name mapping\n",
    "model_name_dict = {\n",
    "    'baseline_mse': \"PF\",\n",
    "    'SPO': \"SPO\",\n",
    "    'DBB': \"DBB\",\n",
    "    \"IMLE\": \"IMLE\",\n",
    "    'FenchelYoung': \"FY\",\n",
    "    \"DCOL\": \"DCOL\",\n",
    "    'IntOpt': \"IntOpt\",\n",
    "    'CachingPO_listwise': \"Listwise\",\n",
    "    'CachingPO_pairwise': \"Pairwise\",\n",
    "    'CachingPO_pairwise_diff': \"PairwiseDiff\",\n",
    "    'CachingPO_MAP_c': \"MAP\",\n",
    "}\n",
    "\n",
    "# Create a readable model name column for the x-axis\n",
    "data[\"model_name\"] = data[\"params.attacked_models_name\"].map(model_name_dict).fillna(data[\"params.attacked_models_name\"]).astype(str)\n",
    "\n",
    "# Prepare a list of records for a new DataFrame that will hold\n",
    "# every distribution point for each (attacker, run_id, metric) *plus* clean distribution\n",
    "rows_for_boxplot = []\n",
    "\n",
    "# Process both metrics\n",
    "for metric in [\"rel_regret\", \"fool_rel_regret\"]:\n",
    "    # Add clean data only once for rel_regret (since it's the same for both)\n",
    "    if metric == \"rel_regret\":\n",
    "        grouped = data.groupby(\"model_name\")\n",
    "        for name, group_data in grouped:\n",
    "            # get the first entry -> just need the clean val \n",
    "            run_id_one_attacker = group_data[\"run_id\"].values[0]\n",
    "            # Get the values\n",
    "            rre_clean, _, _, _, _ = get_attacker_regret_errors(run_id_one_attacker, problem=DATASET)\n",
    "            for val in rre_clean:\n",
    "                rows_for_boxplot.append({\n",
    "                    \"model_name\": name,\n",
    "                    \"attacker\": \"clean\",\n",
    "                    \"regret_value\": val,\n",
    "                    \"metric\": metric  # Add metric column\n",
    "                })\n",
    "    \n",
    "    # Process attacked data for each metric\n",
    "    for idx, row in data.iterrows():\n",
    "        run_id = row[\"run_id\"]\n",
    "        attacker = row[\"attacker_sig\"]\n",
    "        model_name = row[\"model_name\"]\n",
    "        \n",
    "        # Get the regret errors\n",
    "        _, _, rre_attacked, _, frre_attacked = get_attacker_regret_errors(run_id, problem=DATASET)\n",
    "        \n",
    "        # Collect the attacked distribution based on metric\n",
    "        if metric == \"fool_rel_regret\":\n",
    "            for val in frre_attacked:\n",
    "                rows_for_boxplot.append({\n",
    "                    \"model_name\": model_name,\n",
    "                    \"attacker\": attacker,\n",
    "                    \"regret_value\": val,\n",
    "                    \"metric\": metric  # Add metric column\n",
    "                })\n",
    "        elif metric == \"rel_regret\":\n",
    "            for val in rre_attacked:\n",
    "                rows_for_boxplot.append({\n",
    "                    \"model_name\": model_name,\n",
    "                    \"attacker\": attacker,\n",
    "                    \"regret_value\": val,\n",
    "                    \"metric\": metric  # Add metric column\n",
    "                })\n",
    "\n",
    "# Convert the collected rows into a DataFrame\n",
    "df_box = pd.DataFrame(rows_for_boxplot)\n",
    "\n",
    "# Set the order of model names to match the dictionary order\n",
    "# Filter to only include models that are actually present in the data\n",
    "model_order = [model for model in model_name_dict.values() if model in df_box[\"model_name\"].unique()]\n",
    "df_box[\"model_name\"] = pd.Categorical(df_box[\"model_name\"], categories=model_order, ordered=True)\n",
    "\n",
    "print(df_box.columns)\n",
    "\n",
    "# Set up color scheme\n",
    "colorscheme = px.colors.qualitative.Plotly\n",
    "color_map = {attacker: colorscheme[i] for i, attacker in enumerate(GLOBAL_ATTACKER_ORDER)}\n",
    "color_map[\"clean\"] = \"black\"\n",
    "\n",
    "# Create the faceted boxplot\n",
    "fig = px.box(\n",
    "    df_box,\n",
    "    x=\"model_name\",\n",
    "    y=\"regret_value\",\n",
    "    color=\"attacker\",\n",
    "    facet_col=\"metric\",  # This creates separate subplots for each metric\n",
    "    title=\"Boxplots of RRE distributions (clean vs. attacked) - Both Metrics\",\n",
    "    color_discrete_map=color_map,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    legend_title=\"Attacker\",\n",
    "    xaxis_title=\"Attacked Model\",\n",
    "    template=\"plotly_white\",\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "# Ensure the x-axis follows the model order\n",
    "fig.update_xaxes(categoryorder='array', categoryarray=model_order)\n",
    "\n",
    "# Optional: Update facet titles to be more readable\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THIS\n",
    "epsilon = \"8\"\n",
    "startpoint = -0.1\n",
    "endpoint = 4\n",
    "\n",
    "# Do not change\n",
    "metric = \"fool_rel_regret\"\n",
    "data = load_data(DATASET)\n",
    "# Now filter the data for the given attackers \n",
    "data = data[data[\"params.epsilon\"] == epsilon]\n",
    "data = data[(data[\"params.max_iter\"] == \"100\") | (data[\"params.max_iter\"].isna()) | (data[\"params.max_iter\"] == \"50\")]\n",
    "# filte only for the apgd attacker and targeted\n",
    "#data = data[data[\"params.attacker\"].str.contains(\"apgd\") | data[\"params.attacker\"].str.contains(\"argeted\")]\n",
    "runs = data.copy()\n",
    "\n",
    "\n",
    "# Now create the CCDF plot\n",
    "# Now load all the runs in this experiment\n",
    "runs[\"attacker_sig\"] = runs.apply(lambda row: \n",
    "    row[\"params.attacker\"] + \n",
    "    \"_e\" + safe_get_param(row, \"params.epsilon\") +\n",
    "    \"_a\" + safe_get_param(row, \"params.alpha\") +\n",
    "    \"_m\" + safe_get_param(row, \"params.max_iter\") +\n",
    "    \"_r\" + safe_get_param(row, \"params.restarts\") +\n",
    "    (\"_s\" + safe_get_param(row, \"params.use_signed_grad\") if \"params.use_signed_grad\" in row else \"\"), \n",
    "    axis=1)\n",
    "\n",
    "# Get all unique attacker signatures for consistent colors across subplots\n",
    "all_attacker_sigs = runs[runs[\"params.epsilon\"] == epsilon][\"attacker_sig\"].unique()\n",
    "color_dict = {attacker: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] \n",
    "              for i, attacker in enumerate(all_attacker_sigs)}\n",
    "\n",
    "# Create a figure with subplots for each epsilon value\n",
    "# Get the number of unique models that are attacked\n",
    "unique_models = runs[\"params.attacked_models_name\"].unique()\n",
    "rows = (len(unique_models) + 3) // 4  # Calculate needed rows (ceiling division)\n",
    "fig = make_subplots(rows=rows, cols=4, subplot_titles=list(unique_models))\n",
    "\n",
    "# Filter data for this epsilon\n",
    "for i, attacked_modelname in enumerate(unique_models):\n",
    "    plot_data = runs[runs[\"params.epsilon\"] == epsilon].copy()\n",
    "    plot_data = plot_data[plot_data[\"params.attacked_models_name\"] == attacked_modelname]\n",
    "    \n",
    "    # Get the row and column to add the trace to\n",
    "    row_num = i // 4 + 1\n",
    "    col_num = i % 4 + 1\n",
    "    \n",
    "    # For each attacker create a CCDF line and add it to the subplot\n",
    "    for _, row in plot_data.iterrows():\n",
    "        rre_clean, _, rre_adv, _, frre_adv = get_attacker_regret_errors(row[\"run_id\"], problem=DATASET)\n",
    "        # Compute the CCDF (1 - empirical CDF)\n",
    "        x = np.linspace(startpoint, endpoint, 1000)\n",
    "        y = []\n",
    "        for x_val in x:\n",
    "            # calculate the fraction of samples greater than x_val \n",
    "            if metric == \"fool_rel_regret\":\n",
    "                threshold_val = get_threshold_val(frre_adv, x_val)\n",
    "            elif metric == \"rel_regret\":\n",
    "                threshold_val = get_threshold_val(rre_adv, x_val)\n",
    "            y.append(threshold_val)\n",
    "        \n",
    "        # Create the CCDF line plot with consistent colors\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode='lines',\n",
    "                name=row['attacker_sig'],\n",
    "                line=dict(color=color_dict[row['attacker_sig']]),\n",
    "                legendgroup=row['attacker_sig'],  # Group by attacker_sig for legend\n",
    "                showlegend=(i == 1),  # Only show in legend for first subplot\n",
    "            ),\n",
    "            row=row_num, col=col_num,\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=f\"CCDF of {metric} for epsilon {epsilon} across all models\",\n",
    "    xaxis_title=f\"Threshold value\",\n",
    "    yaxis_title=\"Fraction of adversarial inputs >= threshold\",\n",
    "    legend_title=\"Attacker\",\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(\n",
    "        groupclick=\"togglegroup\"  # Enable clicking on a legend group to toggle all traces\n",
    "    ),\n",
    "    # Adjust the height and width of the figure\n",
    "    height=rows*300,\n",
    "    width=1800,\n",
    ")\n",
    "\n",
    "# Update all subplot axes for consistency\n",
    "for i in range(1, rows*4+1):\n",
    "    row = (i-1)//4 + 1\n",
    "    col = (i-1)%4 + 1\n",
    "    if i <= len(unique_models):\n",
    "        fig.update_xaxes(title_text=\"Threshold\", row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"Fraction ≥ threshold\", row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=rows*400,\n",
    "    width=2200,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted Plots for Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In thesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THIS\n",
    "epsilon = \"8\"\n",
    "startpoint = -0.05\n",
    "endpoint = 3.0\n",
    "\n",
    "# Do not change\n",
    "metric = \"fool_rel_regret\"\n",
    "data = load_data(DATASET)\n",
    "# Now filter the data for the given attackers \n",
    "data = data[data[\"params.epsilon\"] == epsilon]\n",
    "data = data[(data[\"params.max_iter\"] == \"100\") | (data[\"params.max_iter\"].isna()) | (data[\"params.max_iter\"] == \"50\")]\n",
    "# filte only for the apgd attacker and targeted\n",
    "data = data[data[\"params.attacker\"].str.contains(\"apgd\") | data[\"params.attacker\"].str.contains(\"argeted\") | (data[\"params.attacker\"].str.contains(\"andom\"))]\n",
    "runs = data.copy()\n",
    "\n",
    "\n",
    "# Now create the CCDF plot\n",
    "# Now load all the runs in this experiment\n",
    "runs[\"attacker_sig\"] = runs.apply(lambda row: \n",
    "    row[\"params.attacker\"] + \n",
    "    \"_e\" + safe_get_param(row, \"params.epsilon\") +\n",
    "    \"_a\" + safe_get_param(row, \"params.alpha\") +\n",
    "    \"_m\" + safe_get_param(row, \"params.max_iter\") +\n",
    "    \"_r\" + safe_get_param(row, \"params.restarts\") +\n",
    "    (\"_s\" + safe_get_param(row, \"params.use_signed_grad\") if \"params.use_signed_grad\" in row else \"\"), \n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attacker_sig_to_alias(sig):\n",
    "    if \"terative\" in sig:\n",
    "        return \"TARGETED\"\n",
    "    elif \"apgd_dec\" in sig:\n",
    "        return \"APGD-TRAIN\"\n",
    "    elif \"apgd_adv_loss_mean\" in sig:\n",
    "        return \"APGD-L2\"\n",
    "    elif \"apgd_adv_loss_enf\" in sig:\n",
    "        return \"APGD-NOTOPT\"\n",
    "    elif \"random\" in sig:\n",
    "        return \"RANDOM\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attacker sig: {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots for each epsilon value\n",
    "desired_order = list(model_name_dict.keys())\n",
    "unique_models_in_data = runs[\"params.attacked_models_name\"].unique()\n",
    "# Keep only models that exist in data, in the desired order\n",
    "unique_models = [model for model in desired_order if model in unique_models_in_data]\n",
    "\n",
    "all_attacker_sigs = runs[runs[\"params.epsilon\"] == epsilon][\"attacker_sig\"].unique()\n",
    "all_attacker_sigs = sorted(all_attacker_sigs, key=lambda sig: attacker_sig_to_alias(sig))\n",
    "color_dict = {attacker: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] \n",
    "              for i, attacker in enumerate(all_attacker_sigs)}\n",
    "rows = (len(unique_models) + 2) // 2  # Calculate needed rows (ceiling division)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=rows, cols=2, \n",
    "    subplot_titles=list(unique_models),\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    "    #x_title=\"Threshold value\",  # Shared x-axis title\n",
    "    #y_title=\"Fraction of adversarial inputs ≥ threshold\",  # Shared y-axis title\n",
    "    horizontal_spacing=0.025,\n",
    "    vertical_spacing=0.025\n",
    ")\n",
    "\n",
    "# Filter data for this epsilon\n",
    "for i, attacked_modelname in enumerate(unique_models):\n",
    "    plot_data = runs[runs[\"params.epsilon\"] == epsilon].copy()\n",
    "    plot_data = plot_data[plot_data[\"params.attacked_models_name\"] == attacked_modelname]\n",
    "    # sort the plot data by attacker alphabetically\n",
    "    plot_data = plot_data.sort_values(\"attacker_sig\")\n",
    "\n",
    "    # Get the row and column to add the trace to\n",
    "    row_num = i // 2 + 1\n",
    "    col_num = i % 2 + 1\n",
    "    \n",
    "    # For each attacker create a CCDF line and add it to the subplot\n",
    "    for _, row in plot_data.iterrows():\n",
    "        rre_clean, _, rre_adv, _, frre_adv = get_attacker_regret_errors(row[\"run_id\"], problem=DATASET)\n",
    "        x = np.linspace(startpoint, endpoint, 1000)\n",
    "        y = []\n",
    "        for x_val in x:\n",
    "            # calculate the fraction of samples greater than x_val \n",
    "            if metric == \"fool_rel_regret\":\n",
    "                threshold_val = get_threshold_val(frre_adv, x_val)\n",
    "            elif metric == \"rel_regret\":\n",
    "                threshold_val = get_threshold_val(rre_adv, x_val)\n",
    "            y.append(threshold_val)\n",
    "        \n",
    "        # Create the CCDF line plot with consistent colors\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode='lines',\n",
    "                name=attacker_sig_to_alias(row['attacker_sig']),\n",
    "                line=dict(color=color_dict[row['attacker_sig']]),\n",
    "                legendgroup=row['attacker_sig'],  # Group by attacker_sig for legend\n",
    "                showlegend=(i == 1),  # Only show in legend for first subplot\n",
    "            ),\n",
    "            row=row_num, col=col_num,\n",
    "        )\n",
    "\n",
    "# Then just hide labels except on edges\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False) \n",
    "fig.update_xaxes(showticklabels=True, row=rows)  # Bottom row only\n",
    "fig.update_yaxes(showticklabels=True, col=1)    # Left column only\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    width=600,\n",
    "    showlegend=False,\n",
    "    template=\"plotly_white\",\n",
    "    #margin=dict(l=50, r=20, t=40, b=50)\n",
    ")\n",
    "\n",
    "# Update the titles \n",
    "for i, annotation in enumerate(fig.layout.annotations):\n",
    "    if annotation.text in model_name_dict:\n",
    "        fig.layout.annotations[i].text = model_name_dict[annotation.text]\n",
    "fig.update_annotations(font=dict(size=10))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        x=0.60,  # Right edge (0-1 scale)\n",
    "        y=-0.01,  # Bottom edge (0-1 scale)\n",
    "        xanchor='left',\n",
    "        yanchor='bottom',\n",
    "        bgcolor='rgba(255, 255, 255, 0.8)',  # Semi-transparent white background\n",
    "        bordercolor='rgba(0, 0, 0, 0.2)',\n",
    "        borderwidth=1,\n",
    "        font=dict(size=9),  # Smaller font to save space\n",
    "        tracegroupgap=5,  # Reduce space between items (default is ~10-15)\n",
    "\n",
    "    ),\n",
    "    # Reduce margins significantly\n",
    "    margin=dict(\n",
    "        l=50,   # Left margin\n",
    "        r=20,   # Right margin (reduced since legend is inside plot area)\n",
    "        t=30,   # Top margin\n",
    "        b=50    # Bottom margin (for x-axis label)\n",
    "    ),\n",
    "    # Optional: make figure more compact\n",
    "    height=900,  # Reduce if needed\n",
    "    width=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define path\n",
    "path = \"\"\n",
    "fig.write_image(path, format=\"pdf\", width=600, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLES for the THESIS\n",
    "# DEFINE THIS \n",
    "metric = \"metrics.mean_rel_regret\" # metrics.mean_fool_rel_regret\n",
    "data = load_data(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from adv_error_metrics import adv_relative_regret_error\n",
    "from helpers import get_min_or_max_problem\n",
    "\n",
    "\n",
    "eps = \"8\"\n",
    "attacker_sig = \"random_noise_e8_aNone_mNone_r10_sNone\"\n",
    "show_percentiles = False\n",
    "text_bf = False\n",
    "problem = DATASET\n",
    "\n",
    "\n",
    "# filter for one epsilon\n",
    "data_table = data[data[\"params.epsilon\"] == eps]\n",
    "if attacker_sig != \"Clean_Model\":\n",
    "    data_table = data_table[data_table[\"attacker_sig\"] == attacker_sig]\n",
    "if attacker_sig == \"Clean_Model\":\n",
    "    # In this case it does not matter which attacker we take\n",
    "    # Just group and take the first group \n",
    "    any_to_filter = data_table[\"attacker_sig\"].unique()[0]\n",
    "    data_table = data_table[data_table[\"attacker_sig\"] == any_to_filter]\n",
    "\n",
    "# Now make sure the entries are ordered based on the models in model_name_dict\n",
    "data_table[\"model_order\"] = data_table[\"params.attacked_models_name\"].apply(lambda x\n",
    "    : list(model_name_dict.keys()).index(x) if x in model_name_dict else np.nan)\n",
    "data_table = data_table.sort_values(\"model_order\")\n",
    "\n",
    "attacker_names = \"\"\n",
    "mean_rre_string = \"\"\n",
    "lower_percentile_rre_string = \"{\\scriptsize \\quad 25th \\%}\"\n",
    "upper_percentile_rre_string = \"{\\scriptsize \\quad 75th \\%}\"\n",
    "if attacker_sig == \"Clean_Model\":\n",
    "    # For each attacker \n",
    "    for idx, row in data_table.iterrows():\n",
    "        attacker_run_id = row[\"run_id\"]\n",
    "        # Download the attacker_data_for_the_run\n",
    "\n",
    "        minimize = get_min_or_max_problem(problem)\n",
    "        # First download the data\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            path = mlflow.artifacts.download_artifacts(\n",
    "                run_id=attacker_run_id,\n",
    "                artifact_path=\"adv_samples/adv_samples.npz\",\n",
    "                dst_path=tmpdir,\n",
    "            )\n",
    "            adv_samples = np.load(open(path, \"rb\"))\n",
    "            path = mlflow.artifacts.download_artifacts(\n",
    "                run_id=attacker_run_id,\n",
    "                artifact_path=\"adv_samples/adv_decisions.npz\",\n",
    "                dst_path=tmpdir,\n",
    "            )\n",
    "            adv_decisions = np.load(open(path, \"rb\"))\n",
    "            # Now compute the relative regrets and regrets\n",
    "            c = adv_samples[\"c\"]\n",
    "            dec = adv_samples[\"dec\"]\n",
    "            dec_hat = adv_decisions[\"dec_hat\"]\n",
    "            dec_adv_hat = adv_decisions[\"dec_adv_hat\"]\n",
    "            rel_regrets = adv_relative_regret_error(\n",
    "                c=c,\n",
    "                dec_adv=dec,\n",
    "                dec_adv_hat=dec_hat,  # we only want the normal regret and not of the adv\n",
    "                minimize=minimize,\n",
    "            )\n",
    "\n",
    "            # Now compute the 25th and 75th percentile\n",
    "            mean_rre = np.mean(rel_regrets)\n",
    "            lower_percentile_rre = np.percentile(rel_regrets, 25)\n",
    "            upper_percentile_rre = np.percentile(rel_regrets, 75)\n",
    "            # Append to the strings\n",
    "            attacker_names += f\" & {row['params.attacked_models_name']}\"\n",
    "            if text_bf:\n",
    "                mean_rre_string += f\" & \\\\textbf{{ {mean_rre:.3f} }}\"\n",
    "            else:\n",
    "                mean_rre_string += f\" & {mean_rre:.3f}\"\n",
    "            formatted_lower = f\"{lower_percentile_rre:.3f}\"\n",
    "            if formatted_lower == \"-0.000\":\n",
    "                formatted_lower = \"0.000\"\n",
    "            lower_percentile_rre_string += f\" &  \\\\scriptsize{{ {formatted_lower} }}\"\n",
    "            upper_percentile_rre_string += f\" &  \\\\scriptsize{{ {upper_percentile_rre:.3f} }}\"\n",
    "else:\n",
    "    # For each attacker \n",
    "    for idx, row in data_table.iterrows():\n",
    "        attacker_run_id = row[\"run_id\"]\n",
    "        # Download the attacker_data_for_the_run\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            path = mlflow.artifacts.download_artifacts(\n",
    "                run_id=attacker_run_id,\n",
    "                artifact_path=\"error_metrics/error_metrics.npz\",\n",
    "                dst_path=tmpdir,\n",
    "            )\n",
    "            adv_samples = np.load(open(path, \"rb\"))\n",
    "            rel_regrets = adv_samples[\"rel_regrets\"]\n",
    "            # Now compute the 25th and 75th percentile\n",
    "            mean_rre = np.mean(rel_regrets)\n",
    "            lower_percentile_rre = np.percentile(rel_regrets, 25)\n",
    "            upper_percentile_rre = np.percentile(rel_regrets, 75)\n",
    "            # Append to the strings\n",
    "            attacker_names += f\" & {row['params.attacked_models_name']}\"\n",
    "            if text_bf:\n",
    "                mean_rre_string += f\" & \\\\textbf{{ {mean_rre:.3f} }}\"\n",
    "            else:\n",
    "                mean_rre_string += f\" & {mean_rre:.3f}\"\n",
    "            formatted_lower = f\"{lower_percentile_rre:.3f}\"\n",
    "            if formatted_lower == \"-0.000\":\n",
    "                formatted_lower = \"0.000\"\n",
    "            lower_percentile_rre_string += f\" &  \\\\scriptsize{{ {formatted_lower} }}\"\n",
    "            upper_percentile_rre_string += f\" &  \\\\scriptsize{{ {upper_percentile_rre:.3f} }}\"\n",
    "# Print the results\n",
    "print(attacker_sig + \"\\\\\\\\\")\n",
    "print()\n",
    "print(attacker_names + \"\\\\\\\\\")\n",
    "print()\n",
    "print(mean_rre_string + \"\\\\\\\\\" )\n",
    "if show_percentiles:\n",
    "    print(lower_percentile_rre_string + \"\\\\\\\\\" )\n",
    "    print(upper_percentile_rre_string + \"\\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = [\"IMLE\" ,\"DBB\"]\n",
    "# DEFINE THIS\n",
    "epsilon = \"8\"\n",
    "startpoint = -0.05\n",
    "endpoint = 4.0\n",
    "\n",
    "# Do not change\n",
    "metric = \"fool_rel_regret\"\n",
    "data = load_data(DATASET)\n",
    "# Now filter the data for the given attackers \n",
    "data = data[data[\"params.epsilon\"] == epsilon]\n",
    "data = data[(data[\"params.max_iter\"] == \"100\") | (data[\"params.max_iter\"].isna()) | (data[\"params.max_iter\"] == \"50\")]\n",
    "# filte only for the apgd attacker and targeted\n",
    "data = data[data[\"params.attacker\"].str.contains(\"apgd\") | data[\"params.attacker\"].str.contains(\"argeted\") | (data[\"params.attacker\"].str.contains(\"andom\"))]\n",
    "runs = data.copy()\n",
    "\n",
    "\n",
    "# Now create the CCDF plot\n",
    "# Now load all the runs in this experiment\n",
    "runs[\"attacker_sig\"] = runs.apply(lambda row: \n",
    "    row[\"params.attacker\"] + \n",
    "    \"_e\" + safe_get_param(row, \"params.epsilon\") +\n",
    "    \"_a\" + safe_get_param(row, \"params.alpha\") +\n",
    "    \"_m\" + safe_get_param(row, \"params.max_iter\") +\n",
    "    \"_r\" + safe_get_param(row, \"params.restarts\") +\n",
    "    (\"_s\" + safe_get_param(row, \"params.use_signed_grad\") if \"params.use_signed_grad\" in row else \"\"), \n",
    "    axis=1)\n",
    "# Create a figure with subplots for each epsilon value\n",
    "desired_order = list(model_name_dict.keys())\n",
    "unique_models_in_data = runs[\"params.attacked_models_name\"].unique()\n",
    "# Keep only models that exist in data, in the desired order\n",
    "unique_models = [model for model in desired_order if model in unique_models_in_data]\n",
    "# Now filter the unique models based on to_plot\n",
    "unique_models = [model for model in unique_models if model_name_dict.get(model, model) in to_plot]\n",
    "\n",
    "\n",
    "\n",
    "all_attacker_sigs = runs[runs[\"params.epsilon\"] == epsilon][\"attacker_sig\"].unique()\n",
    "all_attacker_sigs = sorted(all_attacker_sigs, key=lambda sig: attacker_sig_to_alias(sig))\n",
    "color_dict = {attacker: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)] \n",
    "              for i, attacker in enumerate(all_attacker_sigs)}\n",
    "rows =  1  # Calculate needed rows (ceiling division)\n",
    "#fig = make_subplots(rows=rows, cols=2, subplot_titles=list(unique_models))\n",
    "fig = make_subplots(\n",
    "    rows=rows, cols=2, \n",
    "    subplot_titles=list(unique_models),\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=True,\n",
    "    #x_title=\"Threshold value\",  # Shared x-axis title\n",
    "    #y_title=\"Fraction of adversarial inputs ≥ threshold\",  # Shared y-axis title\n",
    "    horizontal_spacing=0.025,\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "\n",
    "# Filter data for this epsilon\n",
    "for i, attacked_modelname in enumerate(unique_models):\n",
    "    plot_data = runs[runs[\"params.epsilon\"] == epsilon].copy()\n",
    "    plot_data = plot_data[plot_data[\"params.attacked_models_name\"] == attacked_modelname]\n",
    "    # sort the plot data by attacker alphabetically\n",
    "    plot_data = plot_data.sort_values(\"attacker_sig\")\n",
    "\n",
    "    # Get the row and column to add the trace to\n",
    "    row_num = i // 2 + 1\n",
    "    col_num = i % 2 + 1\n",
    "    \n",
    "    # For each attacker create a CCDF line and add it to the subplot\n",
    "    for _, row in plot_data.iterrows():\n",
    "        rre_clean, _, rre_adv, _, frre_adv = get_attacker_regret_errors(row[\"run_id\"], problem=DATASET)\n",
    "        x = np.linspace(startpoint, endpoint, 1000)\n",
    "        y = []\n",
    "        for x_val in x:\n",
    "            # calculate the fraction of samples greater than x_val \n",
    "            if metric == \"fool_rel_regret\":\n",
    "                threshold_val = get_threshold_val(frre_adv, x_val)\n",
    "            elif metric == \"rel_regret\":\n",
    "                threshold_val = get_threshold_val(rre_adv, x_val)\n",
    "            y.append(threshold_val)\n",
    "        \n",
    "        # Create the CCDF line plot with consistent colors\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode='lines',\n",
    "                name=attacker_sig_to_alias(row['attacker_sig']),\n",
    "                line=dict(color=color_dict[row['attacker_sig']]),\n",
    "                legendgroup=row['attacker_sig'],  # Group by attacker_sig for legend\n",
    "                showlegend=(i == 0),  # Only show in legend for first subplot\n",
    "            ),\n",
    "            row=row_num, col=col_num,\n",
    "        )\n",
    "\n",
    "# Show tick labels on both plots since there's only one row\n",
    "fig.update_xaxes(showticklabels=True)  # Show x-axis labels on both plots\n",
    "fig.update_yaxes(showticklabels=True, col=1)    # Left column only for y-axis\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    width=600,\n",
    "    showlegend=False,\n",
    "    template=\"plotly_white\",\n",
    "    #margin=dict(l=50, r=20, t=40, b=50)\n",
    ")\n",
    "\n",
    "# Update the titles \n",
    "for i, annotation in enumerate(fig.layout.annotations):\n",
    "    if annotation.text in model_name_dict:\n",
    "        fig.layout.annotations[i].text = model_name_dict[annotation.text]\n",
    "fig.update_annotations(font=dict(size=10))\n",
    "\n",
    "\n",
    "# fig.update_layout(\n",
    "#     showlegend=True,\n",
    "#     legend=dict(\n",
    "#         x=0.60,  # Right edge (0-1 scale)\n",
    "#         y=-0.01,  # Bottom edge (0-1 scale)\n",
    "#         xanchor='left',\n",
    "#         yanchor='bottom',\n",
    "#         bgcolor='rgba(255, 255, 255, 0.8)',  # Semi-transparent white background\n",
    "#         bordercolor='rgba(0, 0, 0, 0.2)',\n",
    "#         borderwidth=1,\n",
    "#         font=dict(size=9),  # Smaller font to save space\n",
    "#         tracegroupgap=5,  # Reduce space between items (default is ~10-15)\n",
    "\n",
    "#     ),\n",
    "#     # Reduce margins significantly\n",
    "#     margin=dict(\n",
    "#         l=50,   # Left margin\n",
    "#         r=20,   # Right margin (reduced since legend is inside plot area)\n",
    "#         t=30,   # Top margin\n",
    "#         b=50    # Bottom margin (for x-axis label)\n",
    "#     ),\n",
    "#     # Optional: make figure more compact\n",
    "#     height=400,  # Reduce if needed\n",
    "#     width=600\n",
    "# )\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"v\",  # Make legend horizontal\n",
    "        x=1,  # Center horizontally\n",
    "        y=1,  # Position below plots (negative value)\n",
    "        xanchor='right',\n",
    "        yanchor='top',\n",
    "        bgcolor='rgba(255, 255, 255, 0.8)',\n",
    "        bordercolor='rgba(0, 0, 0, 0.2)',\n",
    "        borderwidth=1,\n",
    "        font=dict(size=9),\n",
    "        tracegroupgap=5,\n",
    "    ),\n",
    "    # Increase bottom margin to make room for horizontal legend\n",
    "    margin=dict(\n",
    "        l=50,   \n",
    "        r=20,   \n",
    "        t=30,   \n",
    "        b=50    # Increase bottom margin for legend\n",
    "    ),\n",
    "    height=250,\n",
    "    width=600\n",
    ")\n",
    "\n",
    "\n",
    "# Update the x and y titles for the entire figure\n",
    "fig.update_xaxes(title_text=\"Threshold\", row=1, col=1)  # Left plot\n",
    "fig.update_xaxes(title_text=\"Threshold\", row=1, col=2)  # Right plot\n",
    "fig.update_yaxes(title_text=\"FRRE Exceedance Rate\", row=1, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define path\n",
    "path = \"\"\n",
    "fig.write_image(path, format=\"pdf\", width=600, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIT stats for Targeted Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def get_stats_for_attacker_id(attacker_run_id):\n",
    "    all_hit_target = []\n",
    "    all_increase_regret = []\n",
    "    client = mlflow.MlflowClient()\n",
    "    # Get the attacked models name \n",
    "    attacked_models_name = client.get_run(attacker_run_id).data.params[\"attacked_models_name\"]\n",
    "    print(f\"Attacked Model: {attacked_models_name}\")\n",
    "    # Print the name of the attacker\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        path = mlflow.artifacts.download_artifacts(run_id = attacker_run_id, artifact_path=\"stats/stats.pkl\", dst_path= tmpdir)\n",
    "        with open(path, \"rb\") as f:\n",
    "            stats = pickle.load(f)\n",
    "        for nr, sample in enumerate(stats):\n",
    "            all_hit_target.extend(sample[\"hit_target\"])\n",
    "            for i,regret in enumerate(sample[\"cur_regret\"][1:]):\n",
    "                if regret > sample[\"cur_regret\"][i-1]:\n",
    "                    all_increase_regret.append(True)\n",
    "                else:\n",
    "                    all_increase_regret.append(False)\n",
    "    return np.mean(all_hit_target), np.mean(all_increase_regret)\n",
    "\n",
    "def print_stats_for_all_iterative_attackers():\n",
    "    data = load_data(\"Warcraft\")\n",
    "    iterative_attackers = data[data[\"params.attacker\"] == \"IterativeTargetedRegretMaximizationAttack\"]\n",
    "    for idx, row in iterative_attackers.iterrows():\n",
    "        print(f\"Attacker Sig: {row['attacker_sig']}\")\n",
    "        hit_target, increase_regret = get_stats_for_attacker_id(row[\"run_id\"])\n",
    "        print(f\"Hit Target: {hit_target}, Increase Regret: {increase_regret}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "print_stats_for_all_iterative_attackers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
